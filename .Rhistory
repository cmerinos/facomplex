ggplot2::theme(axis.text.y = ggplot2::element_text(size = 10))
# Línea de umbral
if (!is.null(threshold.line)) {
p <- p +
ggplot2::geom_hline(yintercept = threshold.line, linetype = "dashed", color = threshold.color, size = 1) +
ggplot2::annotate("text", x = 1, y = threshold.line,
label = paste("Threshold =", threshold.line),
vjust = -1, color = threshold.color)
}
print(p)
}
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = "FSI_i",
title = "FSI", sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red")
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = "FSI_i",
title = "FSI", sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
#'   data = fsi.out$FSI_i,
#'   item.col = "Items",
#'   value.col = "FSI_i",
#'   sort.items = "ascending",
#'   reverse.items = FALSE,
#'   theme = "light",
#'   title = "FSI por ítem",
#'   bar.color = "darkgreen",
#'   threshold.line = 0.90
#' )
plot.simplicity <- function(data,
item.col = "Item",
value.col = "Coefficient",
sort.items = c("ascending", "none", "descending"),
reverse.items = FALSE,
theme = c("light", "classic", "minimal"),
title = "Simplicity Index by Item",
bar.color = "blue",
threshold.line = NULL,
threshold.color = "red") {
sort.items <- match.arg(sort.items)
theme <- match.arg(theme)
# Checks
if (!requireNamespace("ggplot2", quietly = TRUE)) stop("Please install the 'ggplot2' package.")
if (!(item.col %in% names(data)) || !(value.col %in% names(data))) {
stop(paste("The data.frame must contain columns:", item.col, "and", value.col))
}
# Determinar el orden base de los ítems según sort.items
levels.base <- switch(
sort.items,
ascending  = data[[item.col]][order(data[[value.col]])],
descending = data[[item.col]][order(-data[[value.col]])],
none       = unique(data[[item.col]])
)
if (reverse.items) levels.base <- rev(levels.base)
# Aplicar como factor
data[[item.col]] <- factor(data[[item.col]], levels = levels.base)
# Tema visual
ggtheme <- switch(theme,
light   = ggplot2::theme_light(),
classic = ggplot2::theme_classic(),
minimal = ggplot2::theme_minimal()
)
# Crear gráfico
p <- ggplot2::ggplot(data, ggplot2::aes(x = .data[[item.col]], y = .data[[value.col]])) +
ggplot2::geom_bar(stat = "identity", fill = bar.color) +
ggplot2::coord_flip() +
ggplot2::labs(title = title, x = "Items", y = "Simplicity Value") +
ggtheme +
ggplot2::theme(axis.text.y = ggplot2::element_text(size = 10))
# Línea de umbral
if (!is.null(threshold.line)) {
p <- p +
ggplot2::geom_hline(yintercept = threshold.line, linetype = "dashed", color = threshold.color, size = 1) +
ggplot2::annotate("text", x = 1, y = threshold.line,
label = paste("Threshold =", threshold.line),
vjust = -1, color = threshold.color)
}
print(p)
}
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = "FSI_i",
title = , sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = ,
title = , sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = "FSI_i",
title = , sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = "FSI_i",
title = , sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
plot.simplicity(data = FSI.out$FSI_i,
item.col = "Items",
value.col = ,
title = , sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
plot.simplicity(data = FSI.out$FSI_i,
item.col = ,
value.col = "FSI_i",
title = , sort.items = "none",
theme = "classic",
bar.color = "black",
threshold.line = .90,
threshold.color = "red", reverse.items = T)
devtools::document()
usethis::use_description()
usethis::use_readme_md()
usethis::use_citation()
usethis::use_citation()
usethis::use_description()
devtools::document()
load("~/GitHub/facomplex/data/fullclean.rda")
usethis::use_readme_md()
#' results <- Chofman(ex1.data)
#'
#' # View the results
#' print(results)
#'
#' @references
#' Hofmann, J. (1977). A coefficient of complexity for factorial structures.
#'  \emph{Psychometrika}, 42(4), 453-464.
#'
#' @export
Hofmann <- function(data) {
# Paso 2: Elevar al cuadrado cada carga factorial en cada fila
datos_cuadrado <- data^2
# Paso 3: Sumar las cargas factoriales elevadas al cuadrado de todas las columnas
hoffman_num <- rowSums(datos_cuadrado)^2
# Paso 5: Elevar a la cuarta potencia cada carga factorial en cada fila
datos_cuarta <- data^4
# Paso 6: Sumar las cargas factoriales elevadas a la cuarta potencia de todas las columnas
hoffman_denom <- rowSums(datos_cuarta)
# Paso 7: Calcular Choff
Choff <- hoffman_num / hoffman_denom
# Paso 8: Modificar Choff como 1/Choff para obtener Choff_R
Choff_R <- 1 / Choff
# Crear el data frame con los resultados
resultado <- data.frame(Hoff = Choff, Hoff_R = Choff_R)
return(round(resultado, 3))
return(resultado)
}
Hofmann(ex1_data)
usethis::use_vignette("intro_facomplex")
devtools::build_vignettes()
last.warning
last.error
devtools::build_vignettes()
devtools::build_vignettes()
#' results <- Hofmann(ex1.data)
#'
#' # View the results
#' print(results)
#'
#' @references
#' Hofmann, J. (1977). A coefficient of complexity for factorial structures.
#'  \emph{Psychometrika}, 42(4), 453-464.
#'
#' @export
Hofmann <- function(data) {
# Paso 2: Elevar al cuadrado cada carga factorial en cada fila
datos_cuadrado <- data^2
# Paso 3: Sumar las cargas factoriales elevadas al cuadrado de todas las columnas
hoffman_num <- rowSums(datos_cuadrado)^2
# Paso 5: Elevar a la cuarta potencia cada carga factorial en cada fila
datos_cuarta <- data^4
# Paso 6: Sumar las cargas factoriales elevadas a la cuarta potencia de todas las columnas
hoffman_denom <- rowSums(datos_cuarta)
# Paso 7: Calcular Choff
Choff <- hoffman_num / hoffman_denom
# Paso 8: Modificar Choff como 1/Choff para obtener Choff_R
Choff_R <- 1 / Choff
# Crear el data frame con los resultados
resultado <- data.frame(Hoff = Choff, Hoff_R = Choff_R)
return(round(resultado, 3))
return(resultado)
}
Hofmann(ex1_data)
devtools::build_vignettes()
devtools::document()
devtools::document()
devtools::document()
devtools::build_vignettes()
browseVignettes("facomplex")
citation("facomplex")
data(bfi, package = "psych")
force(bfi.keys)
force(bfi)
View(bfi.keys)
psych::fa(bfi[, 1:10], nfactors = 2, rotate = "oblimin", fm = "minres")
psych::fa(bfi[, 1:10], nfactors = 2, rotate = "oblimin", fm = "uls")
fa.output <- psych::fa(bfi[, 1:10], nfactors = 2, rotate = "oblimin", fm = "uls")
View(fa.output)
fa.output[["loadings"]]
fa.output$loadings
print(fa.output)
print(fa.output$loadings)
summary(fa.output$loadings)
fa.output[["loadings"]]
as.data.frame(unclass(fa.output$loadings))
#' results <- Hofmann(ex1.data)
#'
#' # View the results
#' print(results)
#'
#' @references
#' Hofmann, J. (1977). A coefficient of complexity for factorial structures.
#'  \emph{Psychometrika}, 42(4), 453-464.
#'
#' @export
Hofmann <- function(data) {
# Paso 2: Elevar al cuadrado cada carga factorial en cada fila
datos_cuadrado <- data^2
# Paso 3: Sumar las cargas factoriales elevadas al cuadrado de todas las columnas
hoffman_num <- rowSums(datos_cuadrado)^2
# Paso 5: Elevar a la cuarta potencia cada carga factorial en cada fila
datos_cuarta <- data^4
# Paso 6: Sumar las cargas factoriales elevadas a la cuarta potencia de todas las columnas
hoffman_denom <- rowSums(datos_cuarta)
# Paso 7: Calcular Choff
Choff <- hoffman_num / hoffman_denom
# Paso 8: Modificar Choff como 1/Choff para obtener Choff_R
Choff_R <- 1 / Choff
# Crear el data frame con los resultados
resultado <- data.frame(Hoff = Choff, Hoff_R = Choff_R)
return(round(resultado, 3))
return(resultado)
}
Hofmann(fa.load)
fa.load <- as.data.frame(unclass(fa_result$loadings))
Hofmann(fa.load)
fa.load <- as.data.frame(unclass(fa.output$loadings))
Hofmann(fa.load)
fa.output
#' BSI_result <- BSI(ex1_data)
#'
#' # Sort by ascending order
#' BSI_result_up <- BSI(ex1_data, sort_items = "up")
#'
#' # Sort by descending order
#' BSI_result_down <- BSI(ex1_data, sort_items = "down")
#'
#' @author Cesar Merino-Soto
#' @export
BSI <- function(data, item_names = NULL, sort_items = NULL) {
# Validar si el argumento es una matriz o data.frame
if (!is.matrix(data) && !is.data.frame(data)) {
stop("El argumento 'data' debe ser una matriz o un data.frame.")
}
# Convertir data.frame a matriz si es necesario
loadings <- as.matrix(data)
# Asegurarse de que los valores sean numéricos
if (!is.numeric(loadings)) {
stop("Los valores de 'data' deben ser numéricos (cargas factoriales).")
}
# Manejar posibles NA en la matriz
if (any(is.na(loadings))) {
warning("Se encontraron valores NA. Se ignorarán en los cálculos.")
loadings[is.na(loadings)] <- 0  # Opcional: reemplazar NA con 0
}
# Calcular BSI por ítem (fila)
bsi_values <- rowSums(loadings^4) / (rowSums(loadings^2)^2)
# Calcular el índice de simplicidad global como promedio de los valores por ítem
simplicity_index <- mean(bsi_values, na.rm = TRUE)
# Si el usuario no proporciona nombres de ítems, generar nombres automáticos
if (is.null(item_names)) {
item_names <- paste0("Item.", seq_along(bsi_values))
}
# Crear un data frame con los valores de BSI
results <- data.frame(
Item = item_names,
BSI.Value = round(bsi_values, 3),
stringsAsFactors = FALSE
)
# Aplicar ordenamiento si el usuario lo indica
if (!is.null(sort_items)) {
if (sort_items == "up") {
results <- results[order(results$BSI.Value), ]
} else if (sort_items == "down") {
results <- results[order(results$BSI.Value, decreasing = TRUE), ]
} else {
stop("El argumento 'sort_items' debe ser 'up', 'down' o NULL.")
}
}
# Retornar los resultados
return(list(
BSI.item = results,
BSI.global = round(simplicity_index, 3)
))
}
BSI(fa.load)
Hofmann(fa.load)
#'                                                   rstarts = 30,
#'                                                   algorithm = "gpa",
#'                                                   std.ov = TRUE))
#'
#' FSI(data = lavInspect(INV.esem.fit, what = "std")$lambda,
#'     items_target = list(f1 = c(1, 2, 3, 4, 5, 6),
#'                         f2 = c(7, 8, 9, 10, 11, 12)))
#'
#' @author Tu Nombre
#' @export
FSI <- function(data, items_target) {
# Convert matrices to data frame
if (is.matrix(data)) {
data <- as.data.frame(data)
}
# Verify that data is a data frame
if (!is.data.frame(data)) {
stop("The argument 'data' must be a data frame or a matrix")
}
# Get row names if available
row_names <- rownames(data)
if (is.null(row_names) || all(row_names == "")) {
row_names <- paste0("FSI_", seq_len(nrow(data)))
}
# Square all factor loadings
data_squared <- data^2
# Initialize lists to store results
FSI_F <- list()
FSI_i <- list()
# Initialize variables for total calculation
SSTF_total <- 0
SS_NTF_total <- 0
# Compute FSI_F for each factor
for (factor in names(items_target)) {
# Get the index of target items for the current factor
target_rows <- items_target[[factor]]
# Compute the sum of squared target items (SSTF)
SSTF <- sum(data_squared[target_rows, factor])
# Compute the sum of squared non-target items (SS-NTF)
all_rows <- 1:nrow(data_squared)
no_target_rows <- setdiff(all_rows, target_rows)
SS_NTF <- sum(data_squared[no_target_rows, factor])
# Compute RATIO
RATIO <- SS_NTF / SSTF
# Compute FSI_F
FSI_F[[factor]] <- 1 - RATIO
# Accumulate for total calculation
SSTF_total <- SSTF_total + SSTF
SS_NTF_total <- SS_NTF_total + SS_NTF
# Compute FSI_i for each target item in the current factor
for (item in target_rows) {
target_loading <- data_squared[item, factor]
sum_non_target <- sum(data_squared[item, setdiff(names(data), factor)])
# Compute FSI_i for the current item
FSI_i[[row_names[item]]] <- 1 - (sum_non_target / target_loading)
}
}
# Compute FSI_total
RATIO_total <- SS_NTF_total / SSTF_total
FSI_total <- 1 - RATIO_total
# Create lists for results
result_list <- list(
FSI_total = FSI_total,
FSI_F = FSI_F,
FSI_i = data.frame(
Items = names(FSI_i),
FSI_i = round(unlist(FSI_i), 3),
row.names = NULL  # Remove automatic row names
)
)
# Format and round results
result_list$FSI_total <- round(result_list$FSI_total, 3)
result_list$FSI_F <- sapply(result_list$FSI_F, round, 3)
return(result_list)
}
fa.load
FSI(data = fa.load, items_target = list(ULS1 = 1,2,3,4,5), ULS2 = 6,7,8,9,10)
FSI(data = fa.load, items_target = list(ULS1 = c(1,2,3,4,5), ULS2 = c(6,7,8,9,10)))
FSI(data = fa.load, items_target = list(ULS1 = c(6,7,8,9,10), ULS2 = c(1,2,3,4,5)))
#' Kaiser, H. F., & Cerny, B. A. (1978). Factor analysis of the image covariance matrix. \emph{Psychological Bulletin}, 85(6), 1272–1284.
#' Kendall, M. G., & Stuart, A. (1969). \emph{The Advanced Theory of Statistics}, Vol. 2. London: Griffin.
#'
#' @examples
#' # Simulated example
#' set.seed(123)
#' loadings <- matrix(runif(30, -1, 1), nrow = 10, ncol = 3)
#' KC(loadings)
#'
#' @export
KC <- function(data, b = 4) {
# Ensure numeric matrix
if (!is.matrix(data)) {
data <- as.matrix(data)
}
if (b <= 0) {
stop("The parameter 'b' must be a positive number.")
}
m <- nrow(data)
p <- ncol(data)
# Compute f_j for each factor
fj_values <- apply(data^2, 2, function(factor) {
term <- sum(factor^(1 / b)) / m
term^(b / 2)
})
ideal_hyperplane_count <- m * (p - 1)
# Output results
cat("Kaiser-Cerny Factor Simplicity Analysis:\n")
cat("- Simplicity index f_j for each factor:\n")
for (i in 1:length(fj_values)) {
cat(paste0("  F", i, ": ", round(fj_values[i], 6), "\n"))
}
cat("\n- Ideal hyperplane count: ", ideal_hyperplane_count, "\n")
}
#'
#' lsi <- LSI(ex1_data)
#' cat("Global LSI:", lsi, "\n")
#'
#'@seealso
#'\code{\link{BSI}} for Bentler’s index, \code{\link{FSI}} for Fleming’s index, and \code{\link{plot_simplicity}}
#'for visual inspection of loadings.
#'
#' @author Cesar Merino-Soto
#' @export
LSI <- function(loadings) {
if (!is.matrix(loadings) && !is.data.frame(loadings)) {
stop("The argument 'loadings' must be a matrix or data frame.")
}
B <- as.matrix(loadings)
r <- ncol(B)
epsilon <- 1e-6
# Weighted complexity per item (not reported)
w_items <- apply(B, 1, function(row) {
sum((row^2 + epsilon) * 10^(row^2)) / r
})
# Global average
w_global <- mean(w_items)
# Minimum possible value (theoretical lower bound)
e <- (1 / r) * sum((1 / (1 + epsilon)) * 10^epsilon)
# Normalized LSI
LSI_global <- (w_global - e) / (1 - e)
return(round(LSI_global, 4))
}
fa.load
LSI(fa.load)
KC(data = fa.load)
fa.output$loadings
unclass(fa.output$loadings)
fa.output[["loadings"]]
##
BSI(fa.load)
devtools::build_vignettes()
LSI(fa.load)
fa.load
#'
#' lsi <- LSI(ex1_data)
#' cat("Global LSI:", lsi, "\n")
#'
#'@seealso
#'\code{\link{BSI}} for Bentler’s index, \code{\link{FSI}} for Fleming’s index, and \code{\link{plot_simplicity}}
#'for visual inspection of loadings.
#'
#' @author Cesar Merino-Soto
#' @export
LSI <- function(loadings) {
if (!is.matrix(loadings) && !is.data.frame(loadings)) {
stop("The argument 'loadings' must be a matrix or data frame.")
}
L <- as.matrix(loadings)
p <- nrow(L)  # Number of items
r <- ncol(L)  # Number of factors
epsilon <- 1e-6
# Paso 1: Normalización por columnas
col_norms <- sqrt(colSums(L^2))
Lc <- sweep(L, 2, col_norms, "/")
# Paso 2: Normalización por filas
row_norms <- sqrt(rowSums(Lc^2))
B <- sweep(Lc, 1, row_norms, "/")
# Paso 3: Cálculo de w (complejidad promedio ponderada)
w <- mean(apply(B, 1, function(row) {
sum((row^2 + epsilon) * 10^(row^2)) / r
}))
# Paso 4: Valor mínimo teórico (e) cuando las cargas están distribuidas uniformemente
bij <- rep(1 / sqrt(r), r)
e <- sum((bij^2 + epsilon) * 10^(bij^2)) / r
# Paso 5: Índice final
LSI_global <- (w - e) / (1 - e)
return(round(LSI_global, 4))
}
LSI(fa.load)
browseVignettes("facomplex")
