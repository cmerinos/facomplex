% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropyFL.R
\name{entropyFL}
\alias{entropyFL}
\title{Entropy Index for Factor Simplicity}
\usage{
entropyFL(loadings_matrix, base = 2, normalized = TRUE, scaled = FALSE, nd = 3)
}
\arguments{
\item{loadings_matrix}{A numeric matrix or data frame of factor loadings, where rows represent items and columns represent factors.}

\item{base}{The logarithmic base used to compute entropy. Default is \code{2}, corresponding to entropy in bits.}

\item{normalized}{Logical. If \code{TRUE} (default), entropy values are normalized to range from 0 to 1.}

\item{scaled}{Logical. If \code{TRUE}, also returns the scaled entropy index and the theoretical minimum entropy (\eqn{H_{min}}). Default is \code{FALSE}.}

\item{nd}{Integer. Number of decimal places to round the results. Default is \code{3}. Use \code{NULL} for no rounding.}
}
\value{
A list with the following components:
\describe{
\item{\code{Hnormalized}}{List with \code{H.items}, \code{H.factors}, and \code{H.total} entropy values.}
\item{\code{Hscaled}}{(If \code{scaled = TRUE}) List with \code{Hmin.items}, \code{Hscaled.items}, and \code{Hscaled.total}.}
}
}
\description{
Computes entropy-based indices to quantify the factorial simplicity or complexity of an Exploratory Factor Analysis (EFA) solution.
The entropy is calculated from the squared factor loadings, interpreted as proportional contributions of each factor to an item (or vice versa).
Entropy is computed at three levels: by item, by factor, and globally.
}
\details{
The function assumes that the squared factor loadings (\eqn{\lambda_{ij}^2}) represent the proportion of common variance
that item \eqn{i} shares with factor \eqn{j}. These are normalized within rows or columns to form pseudo-probability distributions,
over which Shannon entropy is computed.

\strong{1. Entropy by item:}
For each item \eqn{i}, let \eqn{\lambda_{ij}} be its loading on factor \eqn{j}, with \eqn{j = 1, ..., k}. Define:

\deqn{p_{ij} = \frac{\lambda_{ij}^2}{\sum_{j=1}^{k} \lambda_{ij}^2}}

Then the entropy for item \eqn{i} is:

\deqn{H_i = - \sum_{j=1}^{k} p_{ij} \log_b(p_{ij})}

If \code{normalized = TRUE}, the value is divided by \eqn{\log_b(k)} to constrain the index to \link{0, 1}.
Lower values indicate that most of the variance is concentrated in one factor (simple structure); higher values indicate cross-loadings.

\strong{2. Entropy by factor:}
Similarly, for each factor \eqn{j}, entropy is calculated over its squared loadings across items:

\deqn{q_{ij} = \frac{\lambda_{ij}^2}{\sum_{i=1}^{n} \lambda_{ij}^2}}

\deqn{H_j = - \sum_{i=1}^{n} q_{ij} \log_b(q_{ij})}

Normalization is done by dividing by \eqn{\log_b(n)}.
A factor with low entropy loads strongly on only a few items; high entropy suggests broad dispersion across many items.

\strong{3. Total entropy:}
The total entropy (\code{H.total}) is computed by treating the entire matrix of squared loadings as a single probability vector. It represents global factorial complexity.
This value is also equivalent to the scaled entropy for the total structure (\code{H.scaled.total}) when \code{scaled = TRUE}.

\strong{4. Minimum and scaled entropy (optional):}
When \code{scaled = TRUE}, the function returns additional values:
\itemize{
\item \code{Hmin.items}: Theoretical minimum entropy for each item, based on Beisel & Moreteau (1997)
\item \code{Hscaled.items}: Scaled entropy index for each item:
\deqn{H_{scaled} = \frac{H - H_{min}}{H_{max} - H_{min}}}
where \eqn{H_{max} = \log_b(k)}
\item \code{Hscaled.total}: Scaled entropy index for the total structure. This is mathematically equivalent to the normalized \code{H.total}.
}

\strong{Interpretation:}
\itemize{
\item Values near 0 indicate highly simple structures (loadings concentrated in few components).
\item Values near 1 suggest factorial ambiguity or complexity.
\item \code{Hnormalized} compares \eqn{H} to the maximum entropy.
\item \code{Hscaled} compares \eqn{H} relative to its minimum and maximum, offering finer distinctions.
\item Useful to compare different rotation methods, number of factors, or loading patterns.
}

Although no formal cutoff exists, lower entropy values (e.g., < 0.20) typically reflect strong
factorial simplicity, whereas values approaching 1 indicate dispersed or ambiguous loading
patterns. Interpretation should be contextualized with additional indices and visual inspection.
}
\examples{
# Example: items with different factorial complexity
loadings <- matrix(c(
  0.7, 0.0, 0.01,  # simple item
  0.1, 0.2, 0.15,  # moderately complex
  0.4, 0.8, 0.2,   # complex with a dominant factor
  0.4, 0.4, 0.4    # maximally complex (equal loadings)
), nrow = 4, byrow = TRUE)

entropyFL(loadings, scaled = TRUE)

}
\references{
Shannon, C. E. (1948). A mathematical theory of communication. \emph{Bell System Technical Journal}, 27(3), 379--423. \doi{10.1002/j.1538-7305.1948.tb00917.x}
Hofmann, R. J. (1978). Complexity and simplicity as objective indices descriptive of factor solutions. \emph{Multivariate Behavioral Research}, 13(2), 247--250.
Lorenzo-Seva, U. (2003). A factor simplicity index. \emph{Psychometrika}, 68(1), 49--60. \doi{10.1007/BF02296652}
Beisel, J. N., & Moreteau, J.-C. (1997). A new method to estimate the lower bound of the Shannon-Wiener index of diversity. \emph{Ecological Modelling}, 99(1), 99--105.
}
