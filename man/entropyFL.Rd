% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropyFL.R
\name{entropyFL}
\alias{entropyFL}
\title{Entropy Index for Factor Simplicity}
\usage{
entropyFL(
  loadings_matrix,
  base = 2,
  normalized = TRUE,
  scaled = FALSE,
  bounded = TRUE,
  nd = 3
)
}
\arguments{
\item{loadings_matrix}{A numeric matrix or data frame of factor loadings, where rows represent items and columns represent factors.}

\item{base}{The logarithmic base used to compute entropy. Default is \code{2}, corresponding to entropy in bits.}

\item{normalized}{Logical. If \code{TRUE} (default), entropy values are normalized to range from 0 to 1 by dividing by \eqn{\log_b(k)} or \eqn{\log_b(n)}.}

\item{scaled}{Logical. If \code{TRUE}, returns the scaled entropy index and the theoretical minimum entropy as proposed by Beisel & Moreteau (1997). Default is \code{FALSE}.}

\item{bounded}{Logical. If \code{TRUE} (default), forces scaled entropy values to remain within the \link{0, 1} range by truncating negative or >1 values.}

\item{nd}{Integer. Number of decimal places to round the results. Default is \code{3}. Use \code{NULL} for no rounding.}
}
\value{
A list with:
\describe{
\item{\code{Hnormalized}}{A list with entropy by item, factor, and total.}
\item{\code{Hscaled}}{A list with \code{Hmin.items}, \code{Hscaled.items}, and \code{Hscaled.total} (if \code{scaled = TRUE}).}
}

#' \strong{Interpretation:}
\itemize{
\item Values near 0 indicate high factorial simplicity (loadings concentrated on a single factor).
\item Values near 1 suggest factorial complexity or ambiguity (dispersed loadings across factors).
\item \code{Hnormalized} expresses entropy as a proportion of the maximum possible entropy.
\item \code{Hscaled} expresses entropy relative to its theoretical minimum and maximum, allowing finer differentiation across contexts.
\item The index can be used to compare different rotation methods, number of factors, or item structures.
}

Although no formal cutoff exists, entropy values below 0.20 typically reflect strong factorial simplicity,
while values near or above 0.80 may indicate multidimensionality or poor simple structure.
Interpretation should always be contextualized using additional indices, visual inspection of loadings, and substantive theory.
}
\description{
Computes entropy-based indices to quantify the factorial simplicity or complexity of an Exploratory Factor Analysis (EFA) solution.
Entropy is computed at three levels: by item, by factor, and globally. Two types of entropy measures are available: normalized and scaled.
}
\details{
The entropy index is based on the squared factor loadings (\eqn{\lambda_{ij}^2}), interpreted as the proportion of shared variance between item \eqn{i} and factor \eqn{j} (Shannon, 1948).

\strong{1. Normalized Entropy:}
\itemize{
\item For each item \eqn{i}, define the pseudo-proportions:
\deqn{p_{ij} = \frac{\lambda_{ij}^2}{\sum_{j=1}^k \lambda_{ij}^2}}
\item Then compute Shannon entropy:
\deqn{H_i = - \sum_{j=1}^k p_{ij} \log_b(p_{ij})}
\item If \code{normalized = TRUE}, divide by \eqn{\log_b(k)} to constrain values to \link{0, 1}.
}

The same logic applies for factors (across items), replacing \eqn{p_{ij}} with:
\deqn{q_{ij} = \frac{\lambda_{ij}^2}{\sum_{i=1}^n \lambda_{ij}^2}}

\strong{2. Global Entropy:}
Entropy can also be calculated for the full loading matrix as a whole:
\deqn{p_{ij} = \frac{\lambda_{ij}^2}{\sum_{i,j} \lambda_{ij}^2}}
\deqn{H = - \sum_{i,j} p_{ij} \log_b(p_{ij})}
and normalized by \eqn{\log_b(n \cdot k)}.

\strong{3. Scaled Entropy (Beisel & Moreteau, 1997):}
When \code{scaled = TRUE}, the function returns:
\itemize{
\item \eqn{H_{min}}: A theoretical lower bound for entropy when one factor dominates:
\deqn{H_{min} = - [p_{max} \log_b(p_{max}) + (1 - p_{max}) \log_b((1 - p_{max}) / (k - 1))]}
\item \eqn{H_{scaled}}: A scaled measure between \eqn{H_{min}} and \eqn{H_{max}}:
\deqn{H_{scaled} = \frac{H - H_{min}}{H_{max} - H_{min}}}
}

\strong{4. Argument \code{bounded}:}
Scaled entropy can occasionally produce values outside \link{0, 1} if entropy is below the theoretical minimum.
If \code{bounded = TRUE}, the function truncates those values to stay within \link{0, 1} for interpretive clarity.
}
\examples{
# Example: items with different factorial complexity
loadings <- matrix(c(
  0.7, 0.0, 0.01,
  0.1, 0.2, 0.15,
  0.4, 0.8, 0.2,
  0.4, 0.4, 0.4
), nrow = 4, byrow = TRUE)

entropyFL(loadings, normalized = TRUE, scaled = TRUE, bounded = TRUE)

}
\references{
Shannon, C. E. (1948). A mathematical theory of communication. \emph{Bell System Technical Journal}, 27(3), 379–423. \doi{10.1002/j.1538-7305.1948.tb00917.x}

Beisel, J. N., & Moreteau, J.-C. (1997). A new method to estimate the lower bound of the Shannon-Wiener index of diversity. \emph{Ecological Modelling}, 99(1), 99–105.

Hofmann, R. J. (1978). Complexity and simplicity as objective indices descriptive of factor solutions. \emph{Multivariate Behavioral Research}, 13(2), 247–250.

Lorenzo-Seva, U. (2003). A factor simplicity index. \emph{Psychometrika}, 68(1), 49–60. \doi{10.1007/BF02296652}
}
